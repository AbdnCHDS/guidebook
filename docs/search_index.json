[
["index.html", "ACHDS Guidebook Overview Aim Sections This book", " ACHDS Guidebook Overview This guidebook describes the data and coding standards and processes we follow at the Aberdeen Centre for Health Data Science, that enable us to collaborate and create reproducible research. It is being developed by existing team members, to help new members get started. Aim There are many exciting things to consider when starting a new health data science project. For example: What is the purpose of the project? What data do you need to answer your questions? What is the best type of visualisation that will allow you to convey your message? What is the most suitable statistical analysis? The analysis, the visualisation - these are the fun and interesting aspects of the project, that you want to spend all your time doing. So you quickly create a folder called “project”, put your data in a file called “data”, and your analysis code in a file called “analysis”. After a bit of work you save this as “analysis_2”, because you’ve updated it but don’t want to lose the original. When you receive an updated version of the input data, you realise that your analysis code no longer works, so you copy and paste the parts that do work into a new file what you call “new_analysis”… By the end of the project you have a folder full of files with variations of similar, generic names that (hopefully) make perfect sense to you at that point in time. But it is completely unhelpful for everyone else, including yourself a few months into the future. If you had just spent a bit more time organising your project in the beginning, choosing sensible names, keeping good track of your changes, documenting and testing your code, you would have ended up with a project that was not only useful to you just when you finished it, but for many years later, to you and other researchers. But deciding how to organise a project, how to write tests and documentation is not only rather boring, but it’s also not something researchers are taught how to do.That’s why we are all tempted to skip it! So the point of this guidebook is to explain what you need to do and how to do it, to ensure your project can be used and reproduced by others. We hope you find it useful! Sections Our guidebook is work in progress, and everyone is invited to contribute. If you’d like to help improve our guidebook, have a look here: https://github.com/AbdnCHDS/guidebook The sections we have thought about including so far are: How to structure your project folders (to keep data, methods and outputs separate) How to name your files (so they are human &amp; machine readable, and can be ordered well) How to document your data (what information to include about the dataset creators, description, license, variables, etc.) Basic data quality control (what to use for missing values in datasets, standard checks on range of values, value types, number of expected observations, etc.) Good coding practices (documentation, testing, version control) Troubleshooting code and asking for help (i.e. how to create minimal working examples) Collaboration (using Github for hosting projects, and enabling peer review and tracking team contributions) Resources for learning R We are focusing on R, as it is a very commonly used programming language in data science. If you use a different language such as Python, it would be great to add some non-R resources! This book This book is built with Bookdown, based on the psyTeachR book template by Lisa DeBruine. "],
["folders.html", "Chapter 1 Project organisation", " Chapter 1 Project organisation There are many ways to organise your project, but the main recommendation is to keep input data, methods and outputs separate. So the main folder structure should reflect this. 1. Inputs The Inputs folder contains: Input data A metadata file, which is a file that describes the data, for example who created it, when, usage rights, etc. See chapter on data documentation. A definitions file, which explains what each variable in the dataset is It might not be possible to include your actual input data in your project folder. This could be because: It is too large, and it is stored somewhere else online, so there is no reason to copy it locally. In that case, you need to include a clear explanation of where the data is stored, and what you need to do to access it (for example, you may need to create an account). It would be great to include a small subset of the data, to demonstrate what it looks like, and to allow you to develop and test your analysis code on it without having to access the remote data. It is private and cannot be publicly shared (you have permissions to access it in a secure environment such as the Grampian Data Safe Haven). In that case, create a dummy dataset that contains the same variables as the real dataset, but fake data. The purpose of this is again to demonstrate what the real data looks like, and allow you to develop and test your analysis code. 2. Methods The Methods folder contains: Analysis code, including code to input and test the quality of your input data. See chapter on data quality. Documentation of code. It is possible to combine code and documentation in a single document, using tools such as R Markdown. See chapter on code documentation. Code tests, to make sure your code does what you want it to do. See chapter on code testing. 3. Outputs Outputs may be different for each project, depending on the project aims. They could be: A manuscript for publication, including figures, bibliography, etc. A new dataset, which needs to be accompanied by a metadata file and a definitions file, and short examples of code for accessing and using the new dataset A new computer model or analysis method, which should include documentation and examples of use You can create your folder structure on your local computer, but you need a good backup system, and you also need to keep your code under version control (which means keeping track of changes you make). So we recommend you create your project as a Github repository. See chapter on version control. "],
["names.html", "Chapter 2 Naming files and folders", " Chapter 2 Naming files and folders There are three main principles for good file names. They should be: 1. Human readable Choose names that are related to the content of the file. Is it a file of demographic data? Don’t call it “Dimitra’s data.csv”. Call it “demographics.csv”. Use hyphens “-” to separate words so the file names are easier to read. For example, if you have a file with code that loads and cleans your demographic data, you could call it “load-clean-demographics.r”. It’s easier than “loadcleandemographics.r”. And it’s definitely more relevant than “Dimitra’s data analysis.r” 2. Machine readable Avoid spaces, punctuation, and any unusual characters. Use underscores \"_\" as delimiters to separate units of metadata. For example, if you have data from different groups, dates and clinics in separate files, you have three units of metadata (group, date and clinic), and a good file name would be: “2020-08-04_group01_clinic04.csv” 3. Sortable If you have a date in your file name, put the date first, and make sure it’s in the YYYY-MM-DD standard, so your files are ordered chronologically. If you have numbers, pad with zeros (01,02,…,09,10,11,… instead of 1,2,…,9,10,11,…) to make sure they are ordered correctly. "],
["metadata.html", "Chapter 3 Data documentation", " Chapter 3 Data documentation "],
["dataquality.html", "Chapter 4 Data quality", " Chapter 4 Data quality "]
]
